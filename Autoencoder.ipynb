{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f32beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6981 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, UpSampling2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.losses import mean_absolute_error\n",
    "from keras.models import load_model\n",
    "\n",
    "path = \"C:\\\\Users\\\\ankit\\\\Downloads\\\\humanfaces\"\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train = train_datagen.flow_from_directory(path, \n",
    "                                          target_size=(256, 256), \n",
    "                                          batch_size=1000, \n",
    "                                          class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3986b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1fd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(2, (3,3), activation='tanh', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=mean_absolute_error, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f047879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  1\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.0375 - accuracy: 0.6198 - val_loss: 0.0411 - val_accuracy: 0.5606\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.0354 - accuracy: 0.6248 - val_loss: 0.0409 - val_accuracy: 0.5626\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.0344 - accuracy: 0.6225 - val_loss: 0.0402 - val_accuracy: 0.5617\n",
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  2\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 103s 2s/step - loss: 0.0370 - accuracy: 0.6069 - val_loss: 0.0389 - val_accuracy: 0.5945\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 98s 2s/step - loss: 0.0357 - accuracy: 0.6178 - val_loss: 0.0392 - val_accuracy: 0.6379\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 93s 2s/step - loss: 0.0344 - accuracy: 0.6186 - val_loss: 0.0385 - val_accuracy: 0.6432\n",
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  3\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 98s 2s/step - loss: 0.0351 - accuracy: 0.6400 - val_loss: 0.0421 - val_accuracy: 0.6155\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.0334 - accuracy: 0.6368 - val_loss: 0.0426 - val_accuracy: 0.6360\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 102s 2s/step - loss: 0.0325 - accuracy: 0.6382 - val_loss: 0.0416 - val_accuracy: 0.6108\n",
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  4\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 108s 2s/step - loss: 0.0365 - accuracy: 0.6193 - val_loss: 0.0383 - val_accuracy: 0.5874\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.0347 - accuracy: 0.6183 - val_loss: 0.0373 - val_accuracy: 0.5944\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 94s 2s/step - loss: 0.0330 - accuracy: 0.6248 - val_loss: 0.0372 - val_accuracy: 0.6510\n",
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  5\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 99s 2s/step - loss: 0.0353 - accuracy: 0.6065 - val_loss: 0.0398 - val_accuracy: 0.6302\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 124s 3s/step - loss: 0.0333 - accuracy: 0.6216 - val_loss: 0.0389 - val_accuracy: 0.5888\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 95s 2s/step - loss: 0.0319 - accuracy: 0.6211 - val_loss: 0.0389 - val_accuracy: 0.6310\n",
      "(1000, 256, 256, 1)\n",
      "(1000, 256, 256, 2)\n",
      "batch:  6\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 106s 2s/step - loss: 0.0338 - accuracy: 0.6364 - val_loss: 0.0355 - val_accuracy: 0.6088\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.0320 - accuracy: 0.6374 - val_loss: 0.0355 - val_accuracy: 0.5990\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.0309 - accuracy: 0.6209 - val_loss: 0.0350 - val_accuracy: 0.6572\n",
      "(981, 256, 256, 1)\n",
      "(981, 256, 256, 2)\n",
      "batch:  7\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 94s 2s/step - loss: 0.0354 - accuracy: 0.6255 - val_loss: 0.0380 - val_accuracy: 0.6116\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 92s 2s/step - loss: 0.0347 - accuracy: 0.6239 - val_loss: 0.0418 - val_accuracy: 0.6051\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 96s 2s/step - loss: 0.0335 - accuracy: 0.6166 - val_loss: 0.0362 - val_accuracy: 0.6275\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train)):\n",
    "    X =[]\n",
    "    Y =[]\n",
    "    for img in train[i]:\n",
    "      try:\n",
    "          lab = rgb2lab(img)\n",
    "          X.append(lab[:,:,0]) \n",
    "          Y.append(lab[:,:,1:] / 128)\n",
    "      except:\n",
    "         print('error')\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    X = X.reshape(X.shape+(1,))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(\"batch: \", i+1)\n",
    "    model.fit(X,Y,validation_split=0.1, epochs=3, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828c6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Rango.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975c326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Rango.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b211ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 620ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "img1_color=[]\n",
    "\n",
    "img1=img_to_array(load_img(\"C:\\\\Users\\\\ankit\\\\Downloads\\\\brain.jpeg\"))\n",
    "img1 = resize(img1 ,(256, 256))\n",
    "img1_color.append(img1)\n",
    "\n",
    "img1_color = np.array(img1_color, dtype=float)\n",
    "img1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]\n",
    "img1_color = img1_color.reshape(img1_color.shape+(1,))\n",
    "\n",
    "output1 = model.predict(img1_color)\n",
    "output1 = output1*160\n",
    "\n",
    "result = np.zeros((256, 256, 3))\n",
    "result[:,:,0] = img1_color[0][:,:,0]\n",
    "result[:,:,1:] = output1[0]\n",
    "imsave(\"C:\\\\Users\\\\ankit\\\\Downloads\\\\braincol.jpg\", lab2rgb(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
